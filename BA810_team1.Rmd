---
title: 'Team 1: Framingham Heart Study CHD Predictions'
author: "Kaiyu Wang, Chinar Boolchandani, Urvashi Tripathi, Chun Zhou, Ryan Nie, Zhenyang Gai"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## I. Setup
```{r message = FALSE, warning = FALSE}
library(readr)
library(data.table)
library(ggplot2)
library(dplyr)
library(reshape2)
library(glmnet)
library(ROCR)
library(pROC)
library(PRROC)
library(lattice)
library(caret)
library(e1071)
library(randomForest) 
library(corrplot)
library(xgboost)
library(stringr)

CHD <-fread("framingham.csv")
```

## II. Clean Data

### 1. Summary
Summary statistics for all features.

```{r}
summary(CHD)
```

### 2. Replace NA
Missing data is replaced with median since all of them all numerical medical data.

```{r}
education_median<-median(CHD$education,na.rm=TRUE)
CHD[is.na(education),education:=education_median]

cigsPerDay_median<-median(CHD$cigsPerDay,na.rm=TRUE)
CHD[is.na(cigsPerDay),cigsPerDay:=cigsPerDay_median]

BPMeds_median<-median(CHD$BPMeds,na.rm=TRUE)
CHD[is.na(BPMeds),BPMeds:=BPMeds_median]

totChol_median<-median(CHD$totChol,na.rm=TRUE)
CHD[is.na(totChol),totChol:=totChol_median]

glucose_median<-median(CHD$glucose,na.rm=TRUE)
CHD[is.na(glucose),glucose:=glucose_median]

heartRate_median<-median(CHD$heartRate,na.rm=TRUE)
CHD[is.na(heartRate),heartRate:=heartRate_median]

BMI_median<-median(CHD$BMI,na.rm=TRUE)
CHD[is.na(BMI),BMI:=BMI_median]
```

### 3. Rename Column male

```{r}
colnames(CHD)[1] <- 'is_male'
```

## III. Descriptive Data Analysis

### Data Transformation for better data visualization
1. Convert categorical data to dummy variables.
2. Create new columns as feature engineering.
3. Drop unnecessary columns.

```{r}
CHD2 <- CHD %>%
  mutate(is_male = if_else (is_male ==1,"Male","Female"),
         currentSmoker = if_else (currentSmoker ==1,"Smoker","Not a smoker"),
         BPMeds = if_else (BPMeds ==1,"BP meds","No BP meds"),
         prevalentStroke = if_else (prevalentStroke ==1,"Stroke","No Stroke"),
         prevalentHyp = if_else (prevalentHyp ==1,"Hypertensive Yes","Hypertensive No"),
         diabetes = if_else (diabetes ==1,"Has diabetes","No diabetes"),
         TenYearCHD = if_else (TenYearCHD ==1,"Has CHD","No CHD"),
         education = as.factor(education)) %>%
  mutate_if(is.character,as.factor) %>%
  dplyr::select(TenYearCHD,is_male,currentSmoker,BPMeds,prevalentStroke,prevalentHyp,diabetes,everything())
  
#new columns creation
CHD2$BP <- CHD2$sysBP + CHD2$diaBP

#dropping cols
CHD2$sysBP = NULL
CHD2$diaBP = NULL
```

### 1. Distribution of Ten Year Risk of CHD
Labeled data shows only 15% risk of ten year CHD.

```{r}
count1 <- length(which(CHD$TenYearCHD == 1))
cat(count1, "people have a 10 year risk of CHD\n")
count2 <- length(which(CHD$TenYearCHD == 0))
cat(count2, "people DO NOT have a 10 year risk of CHD")
```

```{r}
common_theme <- theme(plot.title = element_text(hjust = 0.5, face = "bold"))

ggplot(data = CHD, aes(x = factor(TenYearCHD), 
                          y = prop.table(stat(count)), 
                          fill = factor(TenYearCHD),
                          label = scales::percent(prop.table(stat(count))))) +
    geom_bar(position = "dodge") + 
    geom_text(stat = 'count',
              position = position_dodge(.9), 
              vjust = -0.5, 
              size =3) + 
    scale_x_discrete(labels = c("no risk", "risk")) +
    scale_y_continuous(labels = scales::percent) +
    labs(x = 'Disease Prevalence', y = 'Percentage',fill='TenYearCHD') +
    ggtitle("Distribution of Ten Year Risk of CHD") +
    common_theme
```

### 2. Distribution of Percentage of CHD with Age
As age increases, the percentage of CHD increases.

```{r}
CHD_a<-data.frame(CHD)
CHD_a$agec <- 
  cut(CHD_a$age, breaks = c(30,35,40,45,50,55,60,65,70),
      labels = c("30-35","35-40","40-45","45-50","50-55","55-60","60-65","65-70"))
  
d <- CHD_a %>% group_by(agec) %>% summarise(perc = mean(TenYearCHD=='1'))
d$perc_r <- round(d$perc,2)*100
d$perc_r <- interaction(d$perc_r, "%", sep = "")
d

ggplot(d, aes(x = agec, y = perc)) +
  geom_col() +
  scale_y_continuous(labels = scales::percent) +
  geom_text(aes(label = perc_r), vjust = -0.5) +
  labs(x = 'Age Group', y = 'Percentage') +
  ggtitle("Distribution of Percentage of CHD with Age") +
  common_theme
```

### 3. Histogram of CHD with Age and Gender
Males are more likely to have CHD than females at earlier ages.

```{r}
CHD_1 <- CHD[ CHD$TenYearCHD=='1',]
CHD_1$is_male[CHD_1$is_male == 0] <- "female"
CHD_1$is_male[CHD_1$is_male == 1] <- "male"
ggplot(data=CHD_1, aes(age,fill=is_male)) +
  geom_bar(position = position_dodge(width = 0.5)) +
  labs(x = "Age in years", y = "CHD count") +
  ggtitle("Distribution of CHD with age and gender") +
  common_theme
```

### 4. Probability of Disease in Smokers
Smoking or not doesn't have significant impact on risk of ten year CHD.

```{r}
d2 <- CHD %>% group_by(currentSmoker) %>% summarise(perc = mean(TenYearCHD=='1'))
d2
```

### 5. Line Chart of Percentage of CHD with Age and Gender

```{r}
d3 <- CHD_a %>% group_by(agec,factor(is_male)) %>% summarise(perc = mean(TenYearCHD=='1'))
d3
ggplot() +
  geom_line(data = d3,
            aes(agec, perc, group = `factor(is_male)`, color = `factor(is_male)`)) +
  scale_y_continuous(labels = scales::percent) +
  labs(x = 'Age Group', y = 'Percentage', color = 'Gender') +
  ggtitle("Distribution of Percentage of CHD with Age and Gender") +
  common_theme
```

### 6. Pairwise Correlation Analysis

```{r}
a <- CHD[,c(1,3,5,7,9,11,13,15)]
pairs(a, col = "coral",main = "Pairwise Correlation Analysis")
```

### 7. Distribution of BMI and Cigarettes per day

```{r}
ggplot(data = CHD, aes(BMI, glucose, color = age)) + 
  geom_point(fill = "blue") 
ggplot(data = CHD, aes(x = cigsPerDay, color = education)) +
  geom_histogram(color = "black", fill = "pink") +
  labs(x = 'Cigarettes per day', y = 'Count of people')
```

### 8. Relationship of BP and Prevalent Hypertension with TenYearCHD

```{r}
CHD2 %>%
  ggplot(aes(x = TenYearCHD, y = BP)) +
  geom_boxplot(fill = 'purple') +
  xlab("Coronary Heart Disease") +
  ylab("Blood Pressure") +
  facet_grid( ~ prevalentHyp) +
  ggtitle("BP and prevalentHyp with TenYearCHD") +
  geom_jitter(
    alpha = 0.5,
    width = 0.2,
    height = 0.2,
    color = "tomato"
  )
```

### 9. Relationship of TotChol and Prevalent Stroke with TenYearCHD

```{r}
CHD2 %>%
  ggplot(aes(x = TenYearCHD, y = totChol)) +
  geom_boxplot(fill = 'orange') +
  xlab("Coronoray Heart Disease") +
  ylab("Total Cholestrol") +
  facet_grid( ~ prevalentStroke) +
  geom_jitter(
    alpha = 0.5,
    width = 0.2,
    height = 0.2,
    color = "tomato"
  ) +
  ggtitle("Cholestrol and prevalentStroke with CHD")
```

### 10. Distribution of TotChol within gender and Prevalent Stroke

```{r}
ggplot(data = CHD2 ,aes(x = education, fill = TenYearCHD)) +
    geom_bar(position = "dodge")+
  ggtitle("TenYearCHD relationship with education")
```

### 11. TenYearCHD relationship with BMI and heartrate

```{r}
ggplot(data = CHD2, 
       aes(x = BMI, y = heartRate, 
           color = TenYearCHD, shape = TenYearCHD)) +
    geom_point() +
    geom_smooth(method = "loess") +
    labs(title = "BMI vs. HeartRate relation with CHD")
```

### 12. TenYearCHD relationship with age and BP

```{r}
ggplot(CHD2, 
       aes(x = age, 
           y = BP, 
           color = TenYearCHD)) +
  geom_point(size = 3, 
             alpha = .6) +
  labs(title = "BP by age related to TenYearCHD")
```

### 13. Correlation plot with significant features

```{r}
cor_heart <- cor(CHD2[,c(8,10:15)])
cor_heart
corrplot(cor_heart,method = 'square',type='upper')
```

## IV. Machine Learning

### 1. Split Dataset

```{r}
library(fastDummies)
CHD_o<-data.frame(CHD)
CHD <-
  dummy_cols(
    CHD,
    select_columns = c(
      'is_male',
      'education',
      'currentSmoker',
      'BPMeds',
      'prevalentStroke',
      'prevalentHyp',
      'diabetes',
      'TenYearCHD'
    ),
    remove_first_dummy = TRUE,
    remove_selected_columns = TRUE
  )
```

```{r}
names(CHD)[names(CHD)=='TenYearCHD_1'] <- 'TenYearCHD'
names(CHD)
```

```{r}
# we decided to do oversampling on our imbalanced data.
library(ROSE)
CHD <- ovun.sample(TenYearCHD ~ ., data = CHD, method = "both", p=0.5,N=2000, seed = 1)$data
CHD_o<-ovun.sample(TenYearCHD ~ ., data = CHD_o, method = "both", p=0.5,N=2000, seed = 1)$data
```

```{r}
set.seed(1)
#train-test split ratio 0.8
id <- createDataPartition(CHD$TenYearCHD, p = 0.8, list = FALSE)
train<-CHD[id, ]
test<-CHD[-id, ]

id_o <- createDataPartition(CHD_o$TenYearCHD, p = 0.8, list = FALSE)
train_o<-CHD_o[id_o, ]
test_o<-CHD_o[-id_o, ]
```

### 2a. Linear Regression Classification

```{r}
train_y <- train_o$TenYearCHD
test_y <- test_o$TenYearCHD
train_x <- train_o[, -16]
test_x <- test_o[, -16]

linearModel <- lm(train_o$TenYearCHD ~ ., train_o)
result <- data.table(predict(linearModel, test_x))
linear_results <- result[,round(V1)]

accuracy_lm <- linear_results + test_y #0 = True negative, #2 = True positive
accuracy <- 1 - (sum(accuracy_lm == 1)/length(accuracy_lm))

cat("The linear regression model accuracy is", accuracy)

cmat <- confusionMatrix(as.factor(linear_results), as.factor(test_y), positive = "1")
cmat

#F1 score
roc.curve(as.numeric(test_y),as.numeric(linear_results))
```

### 2b. Linear Regression with Lasso Classification

```{r}
# Create formula
formula <- as.formula(TenYearCHD ~ .)

# Training set modeling 
train.matrix <- model.matrix(formula, train)[, -1]
train_y <- train$TenYearCHD
fit <- cv.glmnet(train.matrix, train_y, family = "binomial", alpha = 1, nfolds = 10)

# plot 
plot(fit)

# Create testing matrices
test.matrix <- model.matrix(formula, test) [, -1]
```

```{r}
coef(fit, s=fit$lambda.min)
```

```{r}
# Predicting test data

test.predictions <- predict(fit, test.matrix, s = fit$lambda.min, type = "response") 

##F1 score, select cutoff which makes the F1 score largest
Fmeasure <- c()
cutoffs <- seq(0.05, 0.85, 0.01)
for(cutoff in cutoffs) {
  predicted.CHD <- ifelse(test.predictions > cutoff, 1, 0)
  cmat <-
    confusionMatrix(as.factor(predicted.CHD),
                    as.factor(test$TenYearCHD),
                    positive = "1")
  Fmeasure <- c(Fmeasure,  cmat$byClass[7])
}

cutoffs[which.max(Fmeasure)]
#0.15

predicted.CHD <- ifelse(test.predictions > cutoffs[which.max(Fmeasure)], 1, 0)
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

#F1 score
cmat$byClass[7]
roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))
```

### 3. Linear Regression with Ridge Classification

```{r}
# Create formula
formula <- as.formula(TenYearCHD ~ .)

# Training set modeling 
train.matrix <- model.matrix(formula, train)[, -1]
train_y <- train$TenYearCHD
fit <- cv.glmnet(train.matrix, train_y, family = "binomial", alpha = 0, nfolds = 10)
```

```{r}
test.matrix <- model.matrix(formula, test) [, -1]
test.predictions <- predict(fit, test.matrix, s = fit$lambda.min, type = "response") 
predicted.CHD <- ifelse(test.predictions > cutoffs[which.max(Fmeasure)], 1, 0)
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))
```

### 4. Logistic Classification 

```{r}
#use variables selected by lasso
coefs <- coef(fit,s=fit$lambda.min)
variables <- which(coefs !=0)

selectvariables <- names(coefs[variables,])[-1]
selectvariables

train2 <- train.matrix[,selectvariables]
test2 <- test.matrix[,selectvariables]

newtrain <- data.frame(train2, TenYearCHD = train$TenYearCHD)
newtest <- data.frame(test2, TenYearCHD = test$TenYearCHD)

fit2 <- glm(TenYearCHD ~ ., data = newtrain, family = binomial(link = "logit"))
summary(fit2)

# Predicting test data

test.predictions <- predict(fit2, newtest,  type = "response") 

predicted.CHD <- ifelse(test.predictions > cutoffs[which.max(Fmeasure)], 1, 0)
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

#F1 score
cmat$byClass[7]
roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))

#use full data
fit3 <- glm(TenYearCHD ~ ., data = train, family = binomial(link = "logit"))
summary(fit3)

#Predicting test data

test.predictions <- predict(fit3, test,  type = "response") 


predicted.CHD <- ifelse(test.predictions > cutoffs[which.max(Fmeasure)], 1, 0)
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

#F1 score
cmat$byClass[7]
roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))

#use backward selection with AIC criterion

fit4 <- step(fit3,trace = F)
summary(fit4)
#Predicting test data

test.predictions <- predict(fit4, test, type = "response") 

predicted.CHD <- ifelse(test.predictions > cutoffs[which.max(Fmeasure)], 1, 0)
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))

#all logistic models are similar
```

### 5. KNN - K-Nearest Neighbors 

```{r}
set.seed(1)
#set 10-folds cross validation 
ctrl <- trainControl(method = "cv", 
                     number = 10)
#KNN for k-nearest neighbors 

#check parameters tuning results
m <- train(factor(TenYearCHD) ~ ., data = train, 
           method = "knn",
           trControl = ctrl)

m
plot(m)

test.predictions <- predict(m, test,  type = "prob")[,2]

predicted.CHD <- ifelse(test.predictions > cutoffs[which.max(Fmeasure)], 1, 0)
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))
```

### 6. Random Forest

```{r}
set.seed(1)
#set  10-folds cross validation 
ctrl <- trainControl(method = "cv", 
                     number = 10)
#rf for random forest

#check parameters tuning results
m <- train(factor(TenYearCHD) ~ ., data = train, 
           method = "rf",
           trControl = ctrl)
m

plot(m)

#variable important plot
plot(varImp(m))

test.predictions <- predict(m, test,  type = "prob")[,2]

predicted.CHD <- ifelse(test.predictions > cutoffs[which.max(Fmeasure)], 1, 0)
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))
```

### 7. Decision Tree

```{r}
library(rpart)
library(rpart.plot)

train$TenYearCHD <- as.factor(train$TenYearCHD)

fit5 <- rpart(TenYearCHD~., data = train, method = 'class')
rpart.plot(fit5, extra = 106)
```

```{r}
predicted.CHD <- predict(fit5, test, type = 'class')
```

```{r}
cmat <- confusionMatrix(as.factor(predicted.CHD), as.factor(test$TenYearCHD), positive = "1")
cmat

roc.curve(as.numeric(test$TenYearCHD), as.numeric(predicted.CHD))
```

### 8. Extreme Gradient Boosting Model 

```{r}
#Creating a matrix, one-hot encoding for factor variable
training <- sparse.model.matrix(TenYearCHD ~ .-1, data = train)     #independent variable
train_label <-  as.numeric(levels(train$TenYearCHD))[train$TenYearCHD] #dependent variable
train_matrix <- xgb.DMatrix(data = as.matrix(training), label = train_label)

testing <- sparse.model.matrix(TenYearCHD~.-1, data = test)
test_label <- test[,"TenYearCHD"]
test_matrix <- xgb.DMatrix(data = as.matrix(testing), label = test_label)
```

```{r}
#Defining parameters
nc <- length(unique(train_label))
xgb_params <- list("objective" = "multi:softprob",
                   "eval_metric" = "mlogloss",
                   "num_class" = nc)
watchlist <- list(train = train_matrix, test = test_matrix)
```

```{r}
#XGBoost Model
set.seed(333)
best_model <- xgb.train(params = xgb_params,
                       data = train_matrix,
                       nrounds = 100,
                       watchlist = watchlist,
                       eta = 0.001,
                       max.depth = 3,
                       gamma = 0,
                       subsample = 1,
                       colsample_bytree = 1,
                       missing = NA)
```

```{r}
e <- data.frame(best_model$evaluation_log)
plot(e$iter, e$train_mlogloss, col = 'blue')
lines(e$iter, e$test_mlogloss, col = 'red')
```

```{r}
pred <- predict(best_model, newdata = test_matrix)
prediction <- matrix(pred, nrow = nc, ncol = length(pred)/nc) %>%
t() %>%
data.frame() %>%
mutate(label = test_label, max_prob = max.col(., "last")-1)
```

```{r}
cmat <- confusionMatrix(as.factor(prediction$max_prob), as.factor(test$TenYearCHD), positive = "1")
cmat

roc.curve(as.numeric(test$TenYearCHD), as.numeric(prediction$max_prob))
```
